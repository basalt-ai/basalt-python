from dataclasses import dataclass, field
from typing import Any, TypedDict

from .base_log_types import BaseLog, BaseLogParams, LogType


class _PromptReferenceRequired(TypedDict):
    """Required fields for PromptReference."""
    slug: str


class PromptReference(_PromptReferenceRequired, TypedDict, total=False):
    """Reference to a prompt template.

    This class represents a reference to a prompt template used in AI model generations.

    Attributes:
        slug (str): Unique identifier for the prompt template (required).
        version (str): Version of the prompt template (optional).
        tag (str): Tag for the prompt template (optional).

    Example:
        ```python
        # Basic prompt reference
        prompt = PromptReference(slug="qa-prompt", version="2.1.0")
        ```
    """
    version: str | None
    tag: str | None


class GenerationParams(BaseLogParams, total=False):
    """Parameters for creating a new generation.

    This class defines the parameters that can be used to create a new generation,
    either with or without a prompt reference.

    Attributes:
        prompt (Optional[PromptReference]): Reference to the prompt template used.
        input (Optional[str]): The input provided to the model.
        output (Optional[str]): The output generated by the model.
        variables (Optional[Dict[str, Any]]): Variables used in the prompt template.
        options (Optional[Dict[str, Any]]): Additional options for the generation.

    Example:
        ```python
        # Create generation parameters with a prompt reference
        params = GenerationParams(
            name="answer-generation",
            prompt=PromptReference(slug="qa-prompt", version="2.1.0"),
            input="What is the capital of France?",
            variables={"style": "concise", "language": "en"}
        )

        # Create generation parameters without a prompt reference
        params = GenerationParams(
            name="text-completion",
            input="Complete this sentence: The sky is",
            output="The sky is blue and vast."
        )
        ```
    """
    prompt: PromptReference | None
    input: str | None
    output: str | None
    variables: dict[str, Any] | None
    options: dict[str, Any] | None
    input_tokens: int | None
    output_tokens: int | None
    cost: float | None


class UpdateGenerationParams(GenerationParams, total=False):
    """Parameters for updating a generation."""
    name: str | None


@dataclass
class Generation(BaseLog):
    """Generation class representing an AI model generation within a trace.

    This class tracks interactions with AI models, including inputs, outputs,
    and prompt information used for the generation.

    Attributes:
        prompt (Optional[PromptReference]): Reference to the prompt template used.
        input (Optional[str]): The input provided to the model.
        output (Optional[str]): The output generated by the model.
        variables (Optional[Dict[str, Any]]): Variables used in the prompt template.
        type (str): The type of log, defaults to LogType.GENERATION.
        input_tokens (Optional[int]): Number of tokens used for the input.
        output_tokens (Optional[int]): Number of tokens used for the output.
        cost (Optional[float]): Cost of the generation.

    Example:
        ```python
        # Create a generation with a prompt reference
        generation = trace.create_generation(
            name="answer-generation",
            prompt=PromptReference(slug="qa-prompt", version="2.1.0"),
            input="What is the capital of France?"
        )

        # Start the generation
        generation.start()

        # End the generation with output
        generation.end("The capital of France is Paris.")

        # Update generation metadata
        generation.update(metadata={
            "model_version": "gpt-4",
            "tokens_used": 42
        })
        ```
    """
    prompt: PromptReference | None = None
    input: str | None = None
    output: str | None = None
    variables: dict[str, Any] | None = None
    type: LogType = field(default=LogType.GENERATION)
    input_tokens: int | None = None
    output_tokens: int | None = None
    cost: float | None = None

    def start(self, input: str | None = None) -> 'Generation':
        """Marks the generation as started and sets the input if provided.

        Args:
            input (Optional[str]): Optional input data to associate with the generation.

        Returns:
            Generation: The generation instance for method chaining.

        Example:
            ```python
            # Start a generation without input
            generation.start()

            # Start a generation with input
            generation.start("What is the capital of France?")
            ```
        """
        ...

    def end(self, output: str | dict[str, Any] | None = None) -> 'Generation':
        """Marks the generation as ended and sets the output if provided.

        Args:
            output (Optional[Union[str, Dict[str, Any]]]): Optional output data from the model.
                Can be either a string or a dictionary containing output parameters.

        Returns:
            Generation: The generation instance for method chaining.

        Example:
            ```python
            # End a generation without output
            generation.end()

            # End a generation with output as string
            generation.end("The capital of France is Paris.")

            # End a generation with output params
            generation.end({
                "output": "The capital of France is Paris.",
                "input_tokens": 10,
                "output_tokens": 10,
                "cost": 0.01
            })
            ```
        """
        ...

    def update(self, params: 'UpdateGenerationParams') -> 'Generation':
        """Updates the log with new parameters.

        Args:
            **params: The parameters to update.

        Returns:
            The log instance for method chaining.
        """
        ...
