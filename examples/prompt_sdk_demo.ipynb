{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basalt Prompt SDK Demo\n",
    "\n",
    "This notebook demonstrates how to use the Basalt Prompt SDK to interact with your Basalt prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Needed to make notebook work in VSCode\n",
    "\n",
    "os.environ[\"BASALT_BUILD\"] = \"development\"\n",
    "\n",
    "from basalt import Basalt\n",
    "\n",
    "# Initialize the SDK\n",
    "basalt = Basalt(\n",
    "\tapi_key=\"sk-d4ef...\", # Replace with your API key\n",
    "\tlog_level=\"debug\" # Optional: Set log level\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting a Prompt\n",
    "\n",
    "Retrieve a specific prompt using a slug, with optional filters for tag and version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a prompt by slug (default is production version)\n",
    "error, result = basalt.prompt.get('prompt-slug')\n",
    "\n",
    "if error:\n",
    "    print(f\"Error fetching prompt: {error}\")\n",
    "else:\n",
    "    print(f\"Successfully fetched prompt: {result.prompt[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Tags and Versions\n",
    "\n",
    "You can specify a tag or version when retrieving a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a prompt with a specific tag\n",
    "error, result_tag = basalt.prompt.get(slug='prompt-slug', tag='latest')\n",
    "\n",
    "if error:\n",
    "    print(f\"Error fetching prompt with tag: {error}\")\n",
    "else:\n",
    "    print(f\"Successfully fetched prompt with tag 'latest': {result_tag.prompt[:100]}...\")\n",
    "\n",
    "# Get a prompt with a specific version\n",
    "error, result_version = basalt.prompt.get(slug='prompt-slug', version='1.0.0')\n",
    "\n",
    "if error:\n",
    "    print(f\"Error fetching prompt with version: {error}\")\n",
    "else:\n",
    "    print(f\"Successfully fetched prompt with version '1.0.0': {result_version.prompt[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Variables\n",
    "\n",
    "If your prompt has variables, you can pass them when fetching the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a prompt with variables\n",
    "error, result_vars = basalt.prompt.get(\n",
    "    slug='prompt-slug-with-vars', \n",
    "    variables={\n",
    "        'name': 'John Doe',\n",
    "        'role': 'Developer',\n",
    "        'company': 'Acme Inc'\n",
    "    }\n",
    ")\n",
    "\n",
    "if error:\n",
    "    print(f\"Error fetching prompt with variables: {error}\")\n",
    "else:\n",
    "    print(f\"Successfully fetched prompt with variables: {result_vars.prompt[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Prompts with AI Providers\n",
    "\n",
    "Once you have retrieved a prompt, you can use it with your AI provider of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with OpenAI (you'll need to install the openai package)\n",
    "try:\n",
    "    import openai\n",
    "    \n",
    "    # Set up OpenAI client\n",
    "    client = openai.OpenAI(api_key=\"your-openai-api-key\")  # Replace with your OpenAI API key\n",
    "    \n",
    "    # Get a prompt from Basalt\n",
    "    error, result = basalt.prompt.get('prompt-slug')\n",
    "    \n",
    "    if error:\n",
    "        print(f\"Error fetching prompt: {error}\")\n",
    "    else:\n",
    "        # Use the prompt with OpenAI\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": result.prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f\"OpenAI Response: {response.choices[0].message.content}\")\n",
    "except ImportError:\n",
    "    print(\"OpenAI package not installed. Install with: pip install openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Handling\n",
    "\n",
    "Proper error handling when working with prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of handling different error scenarios\n",
    "def get_prompt_safely(slug, tag=None, version=None, variables=None):\n",
    "    try:\n",
    "        error, result = basalt.prompt.get(\n",
    "            slug=slug,\n",
    "            tag=tag,\n",
    "            version=version,\n",
    "            variables=variables\n",
    "        )\n",
    "        \n",
    "        if error:\n",
    "            print(f\"Error fetching prompt '{slug}': {error}\")\n",
    "            return None\n",
    "        return result.prompt\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test with a non-existent prompt\n",
    "prompt_text = get_prompt_safely(\"non-existent-prompt\")\n",
    "print(f\"Result: {prompt_text}\")\n",
    "\n",
    "# Test with a valid prompt\n",
    "prompt_text = get_prompt_safely(\"prompt-slug\")\n",
    "if prompt_text:\n",
    "    print(f\"Valid prompt retrieved: {prompt_text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration with Monitor SDK\n",
    "\n",
    "You can use the Prompt SDK together with the Monitor SDK for a complete workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trace\n",
    "trace = basalt.monitor.create_trace(\n",
    "    \"prompt-workflow\",\n",
    "    {\n",
    "        \"input\": \"Tell me about artificial intelligence\",\n",
    "        \"user\": {\"id\": \"user123\", \"name\": \"Jane Smith\"},\n",
    "        \"metadata\": {\"source\": \"web\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a log for processing\n",
    "processing_log = trace.create_log({\n",
    "    \"type\": \"span\",\n",
    "    \"name\": \"ai-response-generation\",\n",
    "    \"input\": trace.input\n",
    "})\n",
    "\n",
    "# Get a prompt from Basalt\n",
    "error, prompt_result = basalt.prompt.get('ai-explanation-prompt')\n",
    "\n",
    "if error:\n",
    "    processing_log.end({\"error\": str(error)})\n",
    "    trace.end({\"status\": \"error\", \"message\": f\"Failed to get prompt: {error}\"})\n",
    "else:\n",
    "    # Create a generation using the retrieved prompt\n",
    "    generation = processing_log.create_generation({\n",
    "        \"name\": \"ai-explanation\",\n",
    "        \"input\": trace.input,\n",
    "        \"prompt\": prompt_result.prompt\n",
    "    })\n",
    "    \n",
    "    # Simulate AI response\n",
    "    ai_response = \"\"\"\n",
    "    Artificial Intelligence (AI) refers to systems or machines that mimic human intelligence\n",
    "    to perform tasks and can iteratively improve themselves based on the information they collect.\n",
    "    AI manifests in a number of forms including:\n",
    "    \n",
    "    1. Machine Learning\n",
    "    2. Natural Language Processing\n",
    "    3. Computer Vision\n",
    "    4. Robotics\n",
    "    \"\"\"\n",
    "    \n",
    "    # End the generation\n",
    "    generation.end(ai_response)\n",
    "    \n",
    "    # End the log and trace\n",
    "    processing_log.end({\"status\": \"success\", \"output\": ai_response})\n",
    "    trace.end({\"status\": \"success\"})\n",
    "    \n",
    "    print(f\"Generated response using Basalt prompt:\\n{ai_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
