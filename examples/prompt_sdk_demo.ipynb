{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Basalt Prompt SDK Demo\n\nThis notebook demonstrates how to use the Basalt Prompt SDK to interact with your prompts, including:\n- Retrieving prompts with tags and versions\n- Using variables in prompts\n- Integration with AI providers (OpenAI)\n- Telemetry and observability patterns\n\nThis notebook uses primarily mock data for easy execution without API keys."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Needed to make notebook work in VSCode\n",
    "\n",
    "os.environ[\"BASALT_BUILD\"] = \"development\"\n",
    "\n",
    "from basalt import Basalt, TelemetryConfig\n",
    "\n",
    "# Initialize the SDK with telemetry enabled\n",
    "# Note: Replace with your actual API key for real usage\n",
    "telemetry = TelemetryConfig(\n",
    "    service_name=\"prompt-demo\",\n",
    "    environment=\"development\",\n",
    "    enable_llm_instrumentation=True,\n",
    ")\n",
    "\n",
    "basalt = Basalt(\n",
    "    api_key=os.getenv(\"BASALT_API_KEY\", \"sk-demo-key\"),  # Replace with your API key\n",
    "    telemetry_config=telemetry,\n",
    "    trace_user={\"id\": \"demo-user\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting a Prompt\n",
    "\n",
    "Retrieve a specific prompt using a slug, with optional filters for tag and version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a prompt by slug (default is production version)\n",
    "# For real usage, replace 'prompt-slug' with your actual prompt slug\n",
    "try:\n",
    "    result = basalt.prompts.get_sync('prompt-slug')\n",
    "except Exception:\n",
    "    pass\n",
    "    # In production, handle the error appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Tags and Versions\n",
    "\n",
    "You can specify a tag or version when retrieving a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a prompt with a specific tag\n",
    "try:\n",
    "    result_tag = basalt.prompts.get_sync(slug='prompt-slug', tag='latest')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Get a prompt with a specific version\n",
    "try:\n",
    "    result_version = basalt.prompts.get_sync(slug='prompt-slug', version='1.0.0')\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Variables\n",
    "\n",
    "If your prompt has variables, you can pass them when fetching the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a prompt with variables\n",
    "# Variables allow you to customize prompts with dynamic values\n",
    "try:\n",
    "    result_vars = basalt.prompts.get_sync(\n",
    "        slug='prompt-slug-with-vars',\n",
    "        variables={\n",
    "            'name': 'John Doe',\n",
    "            'role': 'Developer',\n",
    "            'company': 'Acme Inc'\n",
    "        }\n",
    "    )\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Prompts with AI Providers\n",
    "\n",
    "Once you have retrieved a prompt, you can use it with your AI provider of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with OpenAI (you'll need to install the openai package)\n",
    "# This demonstrates how to use Basalt prompts with LLM providers\n",
    "try:\n",
    "    import openai\n",
    "\n",
    "    # Set up OpenAI client (replace with your actual API key for real usage)\n",
    "    client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\", \"sk-demo-key\"))\n",
    "\n",
    "    # Get a prompt from Basalt\n",
    "    try:\n",
    "        result = basalt.prompts.get_sync('prompt-slug')\n",
    "\n",
    "        # Use the prompt with OpenAI\n",
    "        # Note: This will fail with demo keys, but shows the pattern\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": result.text}\n",
    "            ]\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Handling\n",
    "\n",
    "Proper error handling when working with prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of handling different error scenarios\n",
    "def get_prompt_safely(slug, tag=None, version=None, variables=None):\n",
    "    \"\"\"\n",
    "    Helper function to safely retrieve prompts with proper error handling.\n",
    "\n",
    "    Returns the prompt text on success, or None on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = basalt.prompts.get_sync(\n",
    "            slug=slug,\n",
    "            tag=tag,\n",
    "            version=version,\n",
    "            variables=variables\n",
    "        )\n",
    "        return result.text\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Test with a non-existent prompt (will fail)\n",
    "prompt_text = get_prompt_safely(\"non-existent-prompt\")\n",
    "if prompt_text:\n",
    "    pass\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# Test with mock valid prompt (will also fail with demo keys)\n",
    "prompt_text = get_prompt_safely(\"prompt-slug\")\n",
    "if prompt_text:\n",
    "    pass\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Integration with Telemetry and Observability\n\nYou can use the Prompt SDK together with Basalt's observability features to trace prompt usage and LLM calls."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basalt.observability.context_managers import trace_generation, trace_span\n",
    "from basalt.observability.decorators import evaluator\n",
    "\n",
    "\n",
    "# Example: Trace a complete prompt-to-LLM workflow\n",
    "@evaluator(\n",
    "    slugs=[\"prompt-quality\", \"response-accuracy\"],\n",
    "    sample_rate=1.0,\n",
    "    metadata=lambda prompt_slug, **kwargs: {\n",
    "        \"prompt_slug\": prompt_slug,\n",
    "        \"source\": \"basalt_prompts\"\n",
    "    }\n",
    ")\n",
    "def generate_response_with_prompt(prompt_slug: str, variables: dict = None) -> str:\n",
    "    \"\"\"\n",
    "    Fetch a prompt and use it to generate a response, with full observability.\n",
    "\n",
    "    The @evaluator decorator will attach evaluators to any instrumented spans\n",
    "    created within this function's context.\n",
    "    \"\"\"\n",
    "    with trace_span(\"prompt_workflow\") as span:\n",
    "        span.set_input({\"prompt_slug\": prompt_slug, \"variables\": variables})\n",
    "\n",
    "        # Step 1: Fetch prompt from Basalt\n",
    "        try:\n",
    "            prompt_result = basalt.prompts.get_sync(slug=prompt_slug, variables=variables)\n",
    "            span.add_event(\"prompt_fetched\")\n",
    "            span.set_attribute(\"prompt.version\", prompt_result.version)\n",
    "\n",
    "            # Step 2: Use prompt with LLM (mocked here)\n",
    "            with trace_generation(\"llm.generate_response\") as llm_span:\n",
    "                llm_span.set_model(\"gpt-4\")\n",
    "                llm_span.set_prompt(prompt_result.text)\n",
    "\n",
    "                # Mock LLM response\n",
    "                mock_response = f\"Mock response using prompt: {prompt_result.slug}\"\n",
    "\n",
    "                llm_span.set_completion(mock_response)\n",
    "                llm_span.set_output({\"response\": mock_response})\n",
    "\n",
    "            span.set_output({\"status\": \"success\", \"response\": mock_response})\n",
    "            return mock_response\n",
    "\n",
    "        except Exception as e:\n",
    "            span.record_exception(e)\n",
    "            span.set_output({\"status\": \"error\", \"error\": str(e)})\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "# Execute the workflow\n",
    "try:\n",
    "    response = generate_response_with_prompt(\n",
    "        prompt_slug=\"example-prompt\",\n",
    "        variables={\"context\": \"demo\"}\n",
    "    )\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Don't forget to shutdown to flush traces\n",
    "# basalt.shutdown()  # Uncomment when done with all operations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}