{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basalt Observability End-to-End Playbook\n",
        "\n",
        "This notebook demonstrates how to combine Basalt's observability helpers with common LLM flows, evaluators,\n",
        "experiment tagging, and Google AI Studio (Gemini) interactions. Each scenario keeps telemetry metadata and\n",
        "evaluators aligned with legacy monitor traces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Install the SDK in editable mode with dev extras: `uv pip install -e \".[dev]\"`\n",
        "- (Optional) Install the Google Generative AI SDK: `pip install google-generativeai`\n",
        "- Set the following environment variables before running the notebook:\n",
        "  - `BASALT_API_KEY` – your Basalt API key\n",
        "  - `GOOGLE_API_KEY` – Google AI Studio key (Gemini)\n",
        "  - `TRACELOOP_TRACE_CONTENT=1` if you want prompts/completions captured\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from basalt import Basalt\n",
        "from basalt.observability import (\n",
        "    add_default_evaluators,\n",
        "    attach_trace_experiment,\n",
        "    configure_trace_defaults,\n",
        "    trace_event,\n",
        "    trace_llm_call,\n",
        "    trace_retrieval,\n",
        "    trace_span,\n",
        "    trace_tool,\n",
        ")\n",
        "from basalt.observability.decorators import trace_llm, trace_operation\n",
        "from basalt.observability.trace_context import TraceContextConfig\n",
        "\n",
        "try:\n",
        "    from google import genai\n",
        "except ImportError:  # pragma: no cover - optional dependency\n",
        "    genai = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configure the Basalt client and default trace context\n",
        "\n",
        "We prime global defaults so that every span carries user, organization, and evaluator metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<basalt.client.Basalt at 0x7f8180392f30>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Configure defaults before instantiating the client\n",
        "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n",
        "\n",
        "from basalt.observability.config import TelemetryConfig\n",
        "\n",
        "configure_trace_defaults(\n",
        "    user={\"id\": \"user-notebook\", \"name\": \"Analyst\"},\n",
        "    organization={\"id\": \"org-research\", \"name\": \"Basalt Research\"},\n",
        "    metadata={\"environment\": \"notebook\", \"workspace\": \"demo\"},\n",
        "    evaluators=[\"accuracy\"],\n",
        ")\n",
        "add_default_evaluators(\"toxicity\")\n",
        "exporter = OTLPSpanExporter(endpoint=\"http://127.0.0.1:4317\", insecure=True)\n",
        "telemetry = TelemetryConfig(service_name=\"notebook\", exporter=exporter)\n",
        "\n",
        "basalt_client = Basalt(\n",
        "    api_key=os.getenv(\"BASALT_API_KEY\", \"not-set\"),\n",
        "    trace_experiment={\"id\": \"exp-observability\", \"feature_slug\": \"demo-agent\"},\n",
        "    trace_metadata={\"notebook\": \"observability-playbook\"},\n",
        "    telemetry_config=telemetry,\n",
        "\n",
        ")\n",
        "\n",
        "basalt_client\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Decorator-driven LLM spans with evaluators\n",
        "\n",
        "The `@trace_llm` decorator grabs prompts, completions, and token usage automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Synthetic monitoring offers a proactive and powerful way to ensure the performance, availability, and reliability of your applications and digital services. Here's a summary of its key benefits:\\n\\n**1. Proactive Problem Detection (Early Warning System):**\\n* **Detects issues before users do:** By simulating user interactions, synthetic monitoring identifies problems before they impact real customers, allowing for faster resolution and preventing customer frustration.\\n* **Reduces downtime:** Catching issues early minimizes the duration of outages, protecting revenue, brand reputation, and user trust.\\n\\n**2. Continuous Availability and Performance Assurance:**\\n* **Constant vigilance:** Synthetic checks run around the clock, providing continuous assurance that your application is up and running and performing as expected.\\n* **Baseline establishment:** It helps establish a baseline for normal performance, making it easier to identify deviations and anomalies.\\n\\n**3. Performance Optimization and Tuning:**\\n* **Identifies performance bottlenecks:** By measuring response times and other performance metrics, it pinpoints areas where your application is slow, enabling targeted optimization efforts.\\n* **Tracks improvements over time:** You can monitor the impact of performance tuning initiatives and ensure that changes are having the desired effect.\\n\\n**4. Geographic and Network Variability Testing:**\\n* **Simulates diverse user locations:** You can set up monitoring points from various geographic locations to understand how users in different regions experience your application.\\n* **Tests across different network conditions:** Simulate different network speeds and latencies to identify performance issues experienced by users with less-than-ideal connections.\\n\\n**5. Enhanced User Experience (UX) Insights:**\\n* **Measures critical user journeys:** By simulating common user workflows (e.g., login, search, checkout), you gain insights into how real users interact with your application and where they might encounter difficulties.\\n* **Ensures a smooth and intuitive experience:** Proactively addressing performance and availability issues directly contributes to a positive user experience.\\n\\n**6. Reduced Mean Time To Resolution (MTTR):**\\n* **Faster root cause analysis:** When an issue is detected, synthetic monitoring provides detailed data and logs, helping teams quickly pinpoint the root cause of the problem.\\n* **Quicker recovery:** Early detection and precise data lead to faster troubleshooting and deployment of fixes, reducing the overall downtime.\\n\\n**7. Cost-Effectiveness and Scalability:**\\n* **Less resource-intensive than real user monitoring (RUM) for certain aspects:** While RUM is essential, synthetic monitoring can be more cost-effective for constant, low-overhead checks of core functionality.\\n* **Scales with your needs:** You can easily add or remove synthetic checks and monitoring locations as your application or business grows.\\n\\n**8. Benchmarking and Competitive Analysis:**\\n* **Compare performance against competitors:** In some cases, synthetic monitoring can be used to anonymously benchmark your application's performance against key competitors.\\n* **Stay ahead of the curve:** Understanding industry performance standards helps you set realistic goals and maintain a competitive edge.\\n\\n**9. Compliance and Service Level Agreement (SLA) Enforcement:**\\n* **Demonstrates uptime and performance:** Provides objective data to prove that your application is meeting agreed-upon SLAs with customers or internal stakeholders.\\n* **Supports audits and compliance requirements:** Generates reports and historical data that can be used for compliance purposes.\\n\\nIn essence, synthetic monitoring is a crucial tool for any organization that relies on the performance and availability of its digital assets. It empowers teams to be proactive, informed, and efficient in maintaining a seamless and positive experience for their users.\""
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@trace_llm(name=\"notebook.gemini.summarize\")\n",
        "def summarize_with_gemini(prompt: str, *, model: str = \"gemini-2.5-flash-lite\") -> str | None:\n",
        "    if genai is None:\n",
        "        raise RuntimeError(\"google-generativeai is not installed\")\n",
        "\n",
        "    client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\", \"fake-key\"))\n",
        "    response = client.models.generate_content(model=model, contents=prompt)\n",
        "    # Convert response to dict so the decorator can introspect usage\n",
        "    return response.text\n",
        "\n",
        "try:\n",
        "    gemini_result = summarize_with_gemini(\"Summarize the benefits of synthetic monitoring.\")\n",
        "except Exception as exc:\n",
        "    gemini_result = {\"error\": str(exc)}\n",
        "\n",
        "gemini_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Manual spans for orchestrating workflow stages\n",
        "\n",
        "We combine `trace_span`, `trace_tool`, and `trace_event` to follow a retrieval-augmented generation pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "with trace_span(\"workflow.rag\", attributes={\"feature\": \"support-bot\"}) as span:\n",
        "    span.add_evaluator(\"latency-budget\")\n",
        "    span.set_experiment(\"exp-rag-001\", feature_slug=\"support-bot\")\n",
        "\n",
        "    with trace_retrieval(\"workflow.rag.retrieve\") as ret_span:\n",
        "        ret_span.set_query(\"error connecting to database\")\n",
        "        ret_span.set_results_count(3)\n",
        "        ret_span.set_top_k(5)\n",
        "\n",
        "    with trace_tool(\"workflow.rag.tool\") as tool_span:\n",
        "        tool_span.set_tool_name(\"web-search\")\n",
        "        tool_span.set_input({\"query\": \"database connection refused troubleshooting\"})\n",
        "        tool_span.set_output({\"summary\": \"Check credentials and firewall rules.\"})\n",
        "\n",
        "    with trace_llm_call(\"workflow.rag.answer\") as llm_span:\n",
        "        llm_span.set_model(\"gemini-1.5-flash\")\n",
        "        llm_span.set_prompt(\"Provide mitigation steps\")\n",
        "        llm_span.set_completion(\"1. Verify credentials...\")\n",
        "        llm_span.set_tokens(input=200, output=180)\n",
        "\n",
        "    with trace_event(\"workflow.rag.event\") as event_span:\n",
        "        event_span.set_event_type(\"handoff\")\n",
        "        event_span.set_payload({\"team\": \"support\", \"status\": \"ready\"})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Experiments and trace enrichment APIs\n",
        "\n",
        "Attach experiment metadata globally, then override within the active span when needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_106982/287295438.py:6: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  span.add_event(\"scoring_started\", {\"timestamp\": datetime.utcnow().isoformat()})\n"
          ]
        }
      ],
      "source": [
        "attach_trace_experiment(\"exp-baseline\", name=\"baseline-playbook\", feature_slug=\"demo-agent\")\n",
        "\n",
        "with trace_span(\"workflow.ab-test\") as span:\n",
        "    span.set_experiment(\"exp-variant\", name=\"variant-b\", feature_slug=\"demo-agent-b\")\n",
        "    span.add_evaluator(\"judge-hallucination\")\n",
        "    span.add_event(\"scoring_started\", {\"timestamp\": datetime.utcnow().isoformat()})\n",
        "    span.set_attribute(\"basalt.metric.latency_ms\", 245)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Shutdown and cleanup\n",
        "\n",
        "Flush telemetry buffers when the workflow completes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "basalt_client.shutdown()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "basalt-sdk",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
